/**
 * AI Fraud Scorer Node - AIé©±åŠ¨çš„èˆå¼Šè¯„åˆ†èŠ‚ç‚¹
 * 
 * æ ¸å¿ƒåŠŸèƒ½ï¼šåŸºäºè§„åˆ™å¼•æ“ + LLMçš„å¤šç»´èˆå¼Šè¯„åˆ†
 * 
 * å®¡è®¡ä»·å€¼ï¼š
 * - è‡ªåŠ¨åŒ–èˆå¼Šé£é™©è¯„ä¼°
 * - ç»“åˆè§„åˆ™å’ŒAIçš„æ··åˆåˆ¤æ–­
 * - ç”Ÿæˆå¯è§£é‡Šçš„é£é™©æŠ¥å‘Š
 * 
 * å¤æ‚åº¦ï¼šHï¼ˆé«˜ï¼‰- AIé›†æˆã€å¤šæ•°æ®æºèåˆ
 */

import { BaseNodeV3, NodeManifest, NodeExecutionResult, NodeExecutionContext } from '../BaseNode';
import type { Records, RiskSet, Evidence, AuditDataType } from '../../../types/AuditDataTypes';

interface FraudScorerConfig {
  sensitivity: 'low' | 'medium' | 'high';
  enableAI: boolean;
  aiModel?: string;
  ruleWeights?: Record<string, number>;
  threshold?: number;
}

interface FraudScore {
  overall: number;              // æ€»ä½“é£é™©åˆ†æ•° 0-100
  dimensions: {
    financial: number;          // è´¢åŠ¡å¼‚å¸¸
    behavioral: number;         // è¡Œä¸ºå¼‚å¸¸
    document: number;          // å•æ®å¼‚å¸¸
    relationship: number;      // å…³ç³»å¼‚å¸¸
  };
  aiReasoning?: string;         // AIæ¨ç†è¿‡ç¨‹
  ruleMatches: Array<{
    rule: string;
    weight: number;
    triggered: boolean;
  }>;
  riskLevel: 'low' | 'medium' | 'high' | 'critical';
}

export class AIFraudScorerNode extends BaseNodeV3 {
  getManifest(): NodeManifest {
    return {
      type: 'ai.fraud_scorer',
      version: '1.0.0',
      category: 'analysis',
      
      label: {
        zh: 'AIèˆå¼Šè¯„åˆ†',
        en: 'AI Fraud Scorer'
      },
      
      description: {
        zh: 'åŸºäºè§„åˆ™å¼•æ“å’Œå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½èˆå¼Šè¯„åˆ†ç³»ç»Ÿã€‚åˆ†æå‡­è¯ã€æµæ°´ã€åˆåŒç­‰å¤šç»´æ•°æ®ï¼Œç”Ÿæˆç»¼åˆé£é™©è¯„åˆ†å’ŒAIæ¨ç†æŠ¥å‘Šã€‚',
        en: 'Intelligent fraud scoring system based on rule engine and LLM. Analyzes vouchers, flows, contracts, and generates comprehensive risk scores with AI reasoning.'
      },
      
      icon: 'ğŸ¤–',
      color: '#9B59B6',
      
      inputs: [
        {
          id: 'vouchers',
          name: 'vouchers',
          type: 'Records',
          required: false,
          description: {
            zh: 'å‡­è¯æ•°æ®',
            en: 'Voucher records'
          }
        },
        {
          id: 'flows',
          name: 'flows',
          type: 'Records',
          required: false,
          description: {
            zh: 'é“¶è¡Œæµæ°´',
            en: 'Bank flows'
          }
        },
        {
          id: 'contracts',
          name: 'contracts',
          type: 'Records',
          required: false,
          description: {
            zh: 'åˆåŒæ•°æ®',
            en: 'Contract data'
          }
        },
        {
          id: 'existingRisks',
          name: 'existingRisks',
          type: 'RiskSet',
          required: false,
          description: {
            zh: 'å·²è¯†åˆ«çš„é£é™©ï¼ˆæ¥è‡ªå…¶ä»–èŠ‚ç‚¹ï¼‰',
            en: 'Existing risks (from other nodes)'
          }
        }
      ],
      
      outputs: [
        {
          id: 'risk',
          name: 'risk',
          type: 'RiskSet',
          required: true,
          description: {
            zh: 'èˆå¼Šé£é™©è¯„åˆ†',
            en: 'Fraud risk assessment'
          }
        },
        {
          id: 'scores',
          name: 'scores',
          type: 'Records',
          required: true,
          description: {
            zh: 'è¯¦ç»†è¯„åˆ†è¡¨',
            en: 'Detailed scores'
          }
        },
        {
          id: 'evidence',
          name: 'evidence',
          type: 'Evidence',
          required: true,
          description: {
            zh: 'AIåˆ†æè¯æ®',
            en: 'AI analysis evidence'
          }
        }
      ],
      
      config: [
        {
          id: 'sensitivity',
          name: { zh: 'æ•æ„Ÿåº¦', en: 'Sensitivity' },
          type: 'select',
          required: false,
          defaultValue: 'medium',
          options: [
            { label: 'ä½', value: 'low' },
            { label: 'ä¸­', value: 'medium' },
            { label: 'é«˜', value: 'high' }
          ],
          description: {
            zh: 'é£é™©æ£€æµ‹æ•æ„Ÿåº¦',
            en: 'Risk detection sensitivity'
          }
        },
        {
          id: 'enableAI',
          name: { zh: 'å¯ç”¨AIåˆ†æ', en: 'Enable AI' },
          type: 'boolean',
          required: false,
          defaultValue: true,
          description: {
            zh: 'æ˜¯å¦ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æ',
            en: 'Whether to use LLM for deep analysis'
          }
        },
        {
          id: 'aiModel',
          name: { zh: 'AIæ¨¡å‹', en: 'AI Model' },
          type: 'select',
          required: false,
          options: [
            { label: 'OpenAI GPT-4', value: 'gpt-4' },
            { label: 'OpenAI GPT-3.5', value: 'gpt-3.5-turbo' },
            { label: 'Qwen Max', value: 'qwen-max' },
            { label: 'Local Model', value: 'local' }
          ],
          description: {
            zh: 'ä½¿ç”¨çš„AIæ¨¡å‹',
            en: 'AI model to use'
          }
        },
        {
          id: 'threshold',
          name: { zh: 'é£é™©é˜ˆå€¼', en: 'Risk Threshold' },
          type: 'number',
          required: false,
          defaultValue: 70,
          description: {
            zh: 'è§¦å‘é«˜é£é™©å‘Šè­¦çš„åˆ†æ•°é˜ˆå€¼ï¼ˆ0-100ï¼‰',
            en: 'Score threshold for high risk alert (0-100)'
          },
          validation: {
            min: 0,
            max: 100
          }
        }
      ],
      
      metadata: {
        author: 'Audit System',
        tags: ['ai', 'fraud-detection', 'scoring', 'llm', 'risk-assessment'],
        documentation: 'https://docs.audit-system.com/nodes/ai/fraud-scorer',
        examples: [
          {
            title: 'AIèˆå¼Šè¯„åˆ†',
            description: 'ç»“åˆå‡­è¯ã€æµæ°´å’ŒAIæ¨ç†è¿›è¡Œèˆå¼Šè¯„åˆ†',
            inputs: {
              vouchers: { type: 'Records', rowCount: 100 },
              flows: { type: 'Records', rowCount: 500 }
            },
            config: {
              sensitivity: 'high',
              enableAI: true,
              aiModel: 'gpt-4'
            }
          }
        ]
      },
      
      capabilities: {
        cacheable: true,
        parallel: true,
        streaming: false,
        aiPowered: true          // æ ‡è®°ä¸ºAIé©±åŠ¨
      }
    };
  }

  async execute(
    inputs: Record<string, AuditDataType>,
    config: Record<string, any>,
    context: NodeExecutionContext
  ): Promise<NodeExecutionResult> {
    const startTime = Date.now();
    
    try {
      const vouchers = inputs.vouchers as Records | undefined;
      const flows = inputs.flows as Records | undefined;
      const contracts = inputs.contracts as Records | undefined;
      const existingRisks = inputs.existingRisks as RiskSet | undefined;
      
      const cfg: FraudScorerConfig = {
        sensitivity: config.sensitivity || 'medium',
        enableAI: config.enableAI !== false,
        aiModel: config.aiModel || 'gpt-3.5-turbo',
        threshold: config.threshold || 70
      };
      
      context.logger?.info?.(`ğŸ¤– Starting AI fraud scoring (AI: ${cfg.enableAI}, Model: ${cfg.aiModel})`);
      
      // 1. è§„åˆ™å¼•æ“è¯„åˆ†ï¼ˆå¿«é€Ÿï¼‰
      const ruleScore = await this.ruleBasedScoring(
        vouchers,
        flows,
        contracts,
        existingRisks,
        cfg,
        context
      );
      
      context.logger?.info?.(`ğŸ“Š Rule-based score: ${ruleScore.overall.toFixed(1)}`);
      
      // 2. AIå¢å¼ºè¯„åˆ†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
      let finalScore = ruleScore;
      if (cfg.enableAI && context.ai) {
        try {
          const aiScore = await this.aiEnhancedScoring(
            ruleScore,
            vouchers,
            flows,
            contracts,
            cfg,
            context
          );
          finalScore = this.mergeScores(ruleScore, aiScore);
          context.logger?.info?.(`ğŸ§  AI-enhanced score: ${finalScore.overall.toFixed(1)}`);
        } catch (error: any) {
          context.logger?.warn?.(`âš ï¸ AI scoring failed, fallback to rules: ${error.message}`);
          // Fallback to rule-based score
        }
      }
      
      // 3. ç”Ÿæˆé£é™©é›†
      const risks = this.generateRisks(finalScore, cfg);
      
      // 4. ç”Ÿæˆè¯æ®
      const evidence = this.generateEvidence(finalScore, context);
      
      // 5. æ„é€ è¯¦ç»†è¯„åˆ†è¡¨
      const scoresRecords: Records = {
        type: 'Records',
        schema: [
          { name: 'dimension', type: 'string', required: true, description: 'Dimension' },
          { name: 'score', type: 'number', required: true, description: 'Score' },
          { name: 'weight', type: 'number', required: true, description: 'Weight' }
        ],
        data: [
          { dimension: 'Financial Anomaly', score: finalScore.dimensions.financial, weight: 0.3 },
          { dimension: 'Behavioral Anomaly', score: finalScore.dimensions.behavioral, weight: 0.25 },
          { dimension: 'Document Anomaly', score: finalScore.dimensions.document, weight: 0.25 },
          { dimension: 'Relationship Anomaly', score: finalScore.dimensions.relationship, weight: 0.2 },
          { dimension: 'Overall', score: finalScore.overall, weight: 1.0 }
        ],
        metadata: this.createMetadata(context.nodeId, context.executionId, 'fraud_scores'),
        rowCount: 5,
        columnCount: 3
      };
      
      const duration = Date.now() - startTime;
      
      context.logger?.info?.(`âœ… AI fraud scoring completed: ${finalScore.overall.toFixed(1)}/100 (${finalScore.riskLevel}) (${duration}ms)`);
      
      return this.wrapSuccess(
        {
          risk: risks,
          scores: scoresRecords,
          evidence
        },
        duration,
        context
      );
      
    } catch (error: any) {
      context.logger?.error?.('âŒ AI fraud scoring failed:', error);
      return this.wrapError('EXECUTION_ERROR', error.message, error.stack);
    }
  }

  // ============================================
  // ç§æœ‰æ–¹æ³•
  // ============================================

  private async ruleBasedScoring(
    vouchers: Records | undefined,
    flows: Records | undefined,
    contracts: Records | undefined,
    existingRisks: RiskSet | undefined,
    config: FraudScorerConfig,
    context: NodeExecutionContext
  ): Promise<FraudScore> {
    const scores = {
      financial: 0,
      behavioral: 0,
      document: 0,
      relationship: 0
    };
    
    const ruleMatches: Array<{ rule: string; weight: number; triggered: boolean }> = [];
    
    // è´¢åŠ¡å¼‚å¸¸è§„åˆ™
    if (flows && flows.rowCount > 0) {
      // è§„åˆ™1: æ•´æ•°é‡‘é¢å¼‚å¸¸ï¼ˆå¤§é‡æ•´ç™¾ã€æ•´åƒï¼‰
      const roundAmounts = flows.data.filter(f => {
        const amount = parseFloat(f.amount || 0);
        return amount % 1000 === 0 || amount % 100 === 0;
      }).length;
      
      const roundRatio = roundAmounts / flows.rowCount;
      if (roundRatio > 0.5) {
        scores.financial += 30;
        ruleMatches.push({ rule: 'High round amount ratio', weight: 30, triggered: true });
      } else if (roundRatio > 0.3) {
        scores.financial += 15;
        ruleMatches.push({ rule: 'Medium round amount ratio', weight: 15, triggered: true });
      }
      
      // è§„åˆ™2: é«˜é¢‘å°é¢äº¤æ˜“
      const smallAmounts = flows.data.filter(f => parseFloat(f.amount || 0) < 5000).length;
      if (smallAmounts / flows.rowCount > 0.7) {
        scores.financial += 20;
        ruleMatches.push({ rule: 'High frequency small transactions', weight: 20, triggered: true });
      }
    }
    
    // å•æ®å¼‚å¸¸è§„åˆ™
    if (vouchers && vouchers.rowCount > 0) {
      // è§„åˆ™3: ç¼ºå°‘é™„ä»¶
      const noAttachment = vouchers.data.filter(v => !v.attachments || v.attachments.length === 0).length;
      const noAttachmentRatio = noAttachment / vouchers.rowCount;
      if (noAttachmentRatio > 0.3) {
        scores.document += 25;
        ruleMatches.push({ rule: 'High missing attachment ratio', weight: 25, triggered: true });
      }
      
      // è§„åˆ™4: å®¡æ‰¹ç¼ºå¤±
      const noApproval = vouchers.data.filter(v => !v.approved_by).length;
      if (noApproval / vouchers.rowCount > 0.1) {
        scores.document += 30;
        ruleMatches.push({ rule: 'Missing approvals', weight: 30, triggered: true });
      }
    }
    
    // è¡Œä¸ºå¼‚å¸¸è§„åˆ™
    if (existingRisks && existingRisks.risks.length > 0) {
      const criticalCount = existingRisks.risks.filter(r => r.severity === 'critical').length;
      const highCount = existingRisks.risks.filter(r => r.severity === 'high').length;
      
      scores.behavioral = Math.min(100, criticalCount * 40 + highCount * 20);
      ruleMatches.push({ rule: 'Existing risk signals', weight: scores.behavioral, triggered: true });
    }
    
    // å…³ç³»å¼‚å¸¸è§„åˆ™
    if (contracts && contracts.rowCount > 0) {
      // ç®€å•ç¤ºä¾‹ï¼šæ£€æŸ¥å…³è”æ–¹
      const relatedParty = contracts.data.filter(c => 
        c.party_type === 'related' || c.relationship === 'related'
      ).length;
      
      if (relatedParty / contracts.rowCount > 0.2) {
        scores.relationship += 35;
        ruleMatches.push({ rule: 'High related party ratio', weight: 35, triggered: true });
      }
    }
    
    // è®¡ç®—æ€»åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
    const overall = 
      scores.financial * 0.3 +
      scores.behavioral * 0.25 +
      scores.document * 0.25 +
      scores.relationship * 0.2;
    
    // æ ¹æ®æ•æ„Ÿåº¦è°ƒæ•´
    const sensitivityMultiplier = config.sensitivity === 'high' ? 1.2 : config.sensitivity === 'low' ? 0.8 : 1.0;
    const adjustedOverall = Math.min(100, overall * sensitivityMultiplier);
    
    return {
      overall: adjustedOverall,
      dimensions: scores,
      ruleMatches,
      riskLevel: this.scoreToLevel(adjustedOverall)
    };
  }

  private async aiEnhancedScoring(
    ruleScore: FraudScore,
    vouchers: Records | undefined,
    flows: Records | undefined,
    contracts: Records | undefined,
    config: FraudScorerConfig,
    context: NodeExecutionContext
  ): Promise<FraudScore> {
    if (!context.ai) {
      return ruleScore;
    }
    
    // å‡†å¤‡AIæç¤ºï¼ˆè„±æ•åçš„æ•°æ®æ‘˜è¦ï¼‰
    const prompt = this.buildAIPrompt(ruleScore, vouchers, flows, contracts);
    
    // è°ƒç”¨AI
    try {
      const response = await context.ai.chat([
        { role: 'system', content: 'You are an expert fraud auditor. Analyze the data and provide risk assessment.' },
        { role: 'user', content: prompt }
      ]);
      
      // è§£æAIå“åº”
      const aiScore = this.parseAIResponse(response);
      
      return {
        ...ruleScore,
        aiReasoning: response
      };
      
    } catch (error: any) {
      context.logger?.warn?.(`AI call failed: ${error.message}`);
      return ruleScore;
    }
  }

  private buildAIPrompt(
    ruleScore: FraudScore,
    vouchers: Records | undefined,
    flows: Records | undefined,
    contracts: Records | undefined
  ): string {
    const parts: string[] = [
      'Analyze the following audit data for fraud risk:',
      '',
      `Rule-based score: ${ruleScore.overall.toFixed(1)}/100`,
      `Dimensions:`,
      `- Financial: ${ruleScore.dimensions.financial}`,
      `- Behavioral: ${ruleScore.dimensions.behavioral}`,
      `- Document: ${ruleScore.dimensions.document}`,
      `- Relationship: ${ruleScore.dimensions.relationship}`,
      ''
    ];
    
    if (vouchers) {
      parts.push(`Vouchers: ${vouchers.rowCount} records`);
    }
    if (flows) {
      parts.push(`Bank flows: ${flows.rowCount} transactions`);
    }
    if (contracts) {
      parts.push(`Contracts: ${contracts.rowCount} contracts`);
    }
    
    parts.push('');
    parts.push('Provide a brief risk assessment (2-3 sentences) focusing on:');
    parts.push('1. Key risk indicators');
    parts.push('2. Potential fraud patterns');
    parts.push('3. Recommended actions');
    
    return parts.join('\n');
  }

  private parseAIResponse(response: string): Partial<FraudScore> {
    // ç®€åŒ–ç‰ˆï¼šå®é™…åº”è¯¥ç”¨æ›´å¤æ‚çš„è§£æ
    return {
      aiReasoning: response
    };
  }

  private mergeScores(ruleScore: FraudScore, aiScore: Partial<FraudScore>): FraudScore {
    // AIåªå¢å¼ºæ¨ç†ï¼Œä¸ä¿®æ”¹åˆ†æ•°ï¼ˆä¿æŒå¯è§£é‡Šæ€§ï¼‰
    return {
      ...ruleScore,
      aiReasoning: aiScore.aiReasoning
    };
  }

  private scoreToLevel(score: number): 'low' | 'medium' | 'high' | 'critical' {
    if (score >= 80) return 'critical';
    if (score >= 60) return 'high';
    if (score >= 40) return 'medium';
    return 'low';
  }

  private generateRisks(score: FraudScore, config: FraudScorerConfig): RiskSet {
    const risks = [{
      id: `fraud_risk_${Date.now()}`,
      category: 'fraud',
      description: `Fraud risk assessment: Overall score ${score.overall.toFixed(1)}/100. ${score.aiReasoning || 'Rule-based analysis.'}`,
      severity: score.riskLevel,
      score: score.overall,
      evidence: [],
      relatedData: {
        dimensions: score.dimensions,
        rules: score.ruleMatches.filter(r => r.triggered)
      },
      suggestedActions: [
        'Review high-risk transactions',
        'Verify document authenticity',
        'Investigate related parties'
      ],
      detectedBy: 'ai_fraud_scorer',
      detectedAt: new Date()
    }];
    
    return {
      type: 'RiskSet',
      risks,
      summary: {
        total: 1,
        bySeverity: {
          [score.riskLevel]: 1
        },
        byCategory: {
          fraud: 1
        }
      },
      metadata: this.createMetadata('', '', 'fraud_assessment')
    };
  }

  private generateEvidence(score: FraudScore, context: NodeExecutionContext): Evidence {
    const evidenceItem: any = {
      id: `evidence_${context.executionId}`,
      type: 'analysis' as const,
      title: 'AI Fraud Scoring Evidence',
      content: {
        overallScore: score.overall,
        riskLevel: score.riskLevel,
        dimensions: score.dimensions,
        ruleMatches: score.ruleMatches.filter(r => r.triggered).map(r => r.rule),
        aiReasoning: score.aiReasoning || 'N/A'
      },
      source: 'ai_fraud_scorer',
      collectedBy: 'ai_fraud_scorer',
      collectedAt: new Date(),
      relatedRisks: [`fraud_risk_${Date.now()}`],
      attachments: [],
      verified: false
    };
    
    return {
      type: 'Evidence',
      items: [evidenceItem],
      traceId: context.executionId,
      workflow: {
        graphId: context.graphId,
        version: '1.0.0',
        nodes: [],
        connections: [],
        timestamp: new Date()
      },
      chain: [],
      metadata: this.createMetadata(context.nodeId, context.executionId, 'fraud_scoring_evidence')
    };
  }
}
